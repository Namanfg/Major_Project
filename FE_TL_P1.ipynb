{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader,Dataset # custom datasets\n",
    "from torchvision import transforms,datasets # mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set display width to avoid truncation\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting things setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda present: cuda\n",
      "name: NVIDIA GeForce MX330\n"
     ]
    }
   ],
   "source": [
    "# check for gpu presence\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "name = torch.cuda.get_device_name(device=None)\n",
    "print(f'cuda present: {device}\\nname: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the path for the data\n",
    "# BASEDIR = 'C:\\\\Users\\\\naman\\\\Downloads\\\\archive\\\\flickr30k_images'\n",
    "# data_path =  os.path.join(BASEDIR,'flickr30k_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the directory of the trail dataset\n",
    "TEST_DIR = 'C:\\\\Python course\\\\Major\\\\Trail_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((600,600)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name  = []\n",
    "class Image_Data_Generator(Dataset):\n",
    "    def __init__(self,directory,transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(directory) if f.endswith(('.png','.jpg','.jpeg'))]\n",
    "        for f in self.image_files:\n",
    "            f = f[:-4]\n",
    "            if f not in img_name:\n",
    "                img_name.append(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        self.image_path = os.path.join(self.directory,self.image_files[idx])\n",
    "        image = Image.open(self.image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Image_Data_Generator(directory=TEST_DIR,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader(data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY RESNET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 23,508,032\n",
      "Trainable params: 23,508,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.55\n",
      "Params size (MB): 89.68\n",
      "Estimated Total Size (MB): 376.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet50(pretrained=True).to(device)\n",
    "resnet = nn.Sequential(*(list(resnet.children())[:-1]))\n",
    "summary(resnet,input_size=(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.eval()\n",
    "features = []\n",
    "with torch.no_grad():\n",
    "    for images,_ in data:\n",
    "        images = images.to(device)\n",
    "        output = resnet(images)\n",
    "        output = output.view(output.size(0),-1)\n",
    "        features.append(output.cpu())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.1023, 0.0851, 0.2017,  ..., 0.3507, 0.2399, 0.1335],\n",
       "         [0.1768, 0.2959, 0.7434,  ..., 0.0290, 0.1907, 0.1949],\n",
       "         [0.1676, 0.4160, 0.7674,  ..., 0.1542, 0.2161, 0.2272],\n",
       "         ...,\n",
       "         [0.1705, 0.4698, 0.5936,  ..., 0.1496, 0.1899, 0.0839],\n",
       "         [0.2335, 0.1446, 1.1984,  ..., 0.0966, 0.2206, 0.1431],\n",
       "         [0.1876, 0.5097, 0.4374,  ..., 0.0830, 0.1382, 0.2505]])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY EFFICIENT NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_net = models.efficientnet_b7(pretrained=True).to(device)\n",
    "efficient_net = nn.Sequential(*(list(efficient_net.children()))[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_net.eval()\n",
    "features = []\n",
    "with torch.no_grad():\n",
    "    for images,_ in data:\n",
    "        images = images.to(device)\n",
    "        output = output.view(output.size(0),-1)\n",
    "        features.append(output.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1023, 0.0851, 0.2017,  ..., 0.3507, 0.2399, 0.1335],\n",
       "        [0.1768, 0.2959, 0.7434,  ..., 0.0290, 0.1907, 0.1949],\n",
       "        [0.1676, 0.4160, 0.7674,  ..., 0.1542, 0.2161, 0.2272],\n",
       "        ...,\n",
       "        [0.1705, 0.4698, 0.5936,  ..., 0.1496, 0.1899, 0.0839],\n",
       "        [0.2335, 0.1446, 1.1984,  ..., 0.0966, 0.2206, 0.1431],\n",
       "        [0.1876, 0.5097, 0.4374,  ..., 0.0830, 0.1382, 0.2505]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = img_name\n",
    "\n",
    "image_features = {img_id : features[0][i] for i, img_id in enumerate(img_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image_ID:985982384 and features: tensor([0.1023, 0.0851, 0.2017,  ..., 0.3507, 0.2399, 0.1335])\n",
      "Image_ID:986127455 and features: tensor([0.1768, 0.2959, 0.7434,  ..., 0.0290, 0.1907, 0.1949])\n",
      "Image_ID:986440271 and features: tensor([0.1676, 0.4160, 0.7674,  ..., 0.1542, 0.2161, 0.2272])\n",
      "Image_ID:987442144 and features: tensor([0.1674, 0.5808, 0.3514,  ..., 0.0687, 0.0309, 0.2093])\n",
      "Image_ID:98756067 and features: tensor([0.1296, 0.4659, 0.6036,  ..., 0.0456, 0.1301, 0.2771])\n",
      "Image_ID:98756125 and features: tensor([0.2528, 0.6382, 0.5387,  ..., 0.0496, 0.0828, 0.1820])\n",
      "Image_ID:98773047 and features: tensor([0.2080, 0.2753, 0.2509,  ..., 0.0487, 0.3861, 0.1570])\n",
      "Image_ID:987907964 and features: tensor([0.1024, 0.9110, 0.3357,  ..., 0.0902, 0.1611, 0.1709])\n",
      "Image_ID:98817947 and features: tensor([0.1434, 0.1556, 0.6955,  ..., 0.0188, 0.1985, 0.1387])\n",
      "Image_ID:98885561 and features: tensor([0.1476, 0.3742, 0.5345,  ..., 0.0282, 0.2009, 0.1559])\n",
      "Image_ID:98944492 and features: tensor([0.0906, 0.3035, 0.2374,  ..., 0.1204, 0.2445, 0.1466])\n",
      "Image_ID:989754491 and features: tensor([0.2852, 0.4606, 0.5140,  ..., 0.1310, 0.2622, 0.3658])\n",
      "Image_ID:989851184 and features: tensor([0.1038, 0.3228, 0.0943,  ..., 0.0943, 0.1923, 0.2662])\n",
      "Image_ID:990890291 and features: tensor([0.2125, 0.4584, 0.1898,  ..., 0.0904, 0.2048, 0.1112])\n",
      "Image_ID:991459749 and features: tensor([0.0702, 0.3477, 0.1673,  ..., 0.0303, 0.0929, 0.0616])\n",
      "Image_ID:99171998 and features: tensor([0.1293, 0.2201, 0.1619,  ..., 0.0327, 0.0471, 0.1482])\n",
      "Image_ID:99458430 and features: tensor([0.1192, 0.5680, 0.1308,  ..., 0.0334, 0.0456, 0.1400])\n",
      "Image_ID:9950858 and features: tensor([0.1320, 0.1180, 0.3547,  ..., 0.0302, 0.0599, 0.1479])\n",
      "Image_ID:9950913 and features: tensor([0.1705, 0.4698, 0.5936,  ..., 0.1496, 0.1899, 0.0839])\n",
      "Image_ID:99679241 and features: tensor([0.2335, 0.1446, 1.1984,  ..., 0.0966, 0.2206, 0.1431])\n",
      "Image_ID:99804383 and features: tensor([0.1876, 0.5097, 0.4374,  ..., 0.0830, 0.1382, 0.2505])\n"
     ]
    }
   ],
   "source": [
    "for i,feat in image_features.items():\n",
    "    print(f'Image_ID:{i} and features: {feat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKING ON THE CAPTION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = {}\n",
    "file = open(\"C:\\\\Python course\\\\Major\\\\result.txt\",\"r\")\n",
    "all_text = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[\\n\\.]','',text)\n",
    "    text = re.sub(\"[^a-z]+\",\" \",text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dictionary = {}\n",
    "for text in all_text:\n",
    "    text = text.split(\"|\")\n",
    "    if text[0].endswith('.jpg'):\n",
    "        temp_list=[]\n",
    "        if text[0][:-4] not in content_dictionary:\n",
    "            clean_text = clean_string(text[-1]) \n",
    "            temp_list.append(clean_text)\n",
    "            content_dictionary[text[0][:-4]] = temp_list\n",
    "        else:\n",
    "            clean_text = clean_string(text[-1])\n",
    "            content_dictionary[text[0][:-4]].append(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dictionary = {img_id:content for img_id,content in content_dictionary.items() if img_id in img_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'985982384': ['some women and men are sun tanning and watching the ocean waves on a bunch of rocks',\n",
       "  'two women relaxing while two men have a conversation on a rock',\n",
       "  'a group of sunbathers lies on the rocks on towels and blankets',\n",
       "  'men and women in swimsuits hangout on rocks above water',\n",
       "  'group of sunbathers laying on the rocks'],\n",
       " '986127455': ['man walking down the street is wearing a black suit and carrying a small white bag',\n",
       "  'a person with a bag walking in a big city',\n",
       "  'people walking a standing in a city park',\n",
       "  'a woman in a black suit is walking by',\n",
       "  'a person is walking with a white bag'],\n",
       " '986440271': ['a young woman in a red flowered dress and a multicolored umbrella is taking a picture of something and there is a large tree in the background',\n",
       "  'a woman in a red floral dress with cardigan takes a photo on a nature trail while holding a rainbow umbrella',\n",
       "  'a woman with a multicolored umbrella takes a picture on a road surrounded by trees',\n",
       "  'a woman with a rainbow umbrella sanding on a road with a handheld camera',\n",
       "  'a woman with a colorful umbrella is taking a picture'],\n",
       " '987442144': ['a young couple prepare a salad together in their kitchen',\n",
       "  'a man and a woman are making two bowls of salad together',\n",
       "  'a man in a striped shirt and a woman are making salads',\n",
       "  'a man and a woman are tossing salads',\n",
       "  'a man and a woman are making salads'],\n",
       " '98756067': ['two men are practicing karate in a room with white brick walls and is in front of some blacked out windows',\n",
       "  'two men practicing a form of martial arts in a wooden floored studio',\n",
       "  'one man violently defeating another man in a matrix style battle',\n",
       "  'two men in white robes and black belts are practicing karate',\n",
       "  'two people are displaying a form of martial arts'],\n",
       " '98756125': ['an asian man practices martial arts in a karate outfit as one hand raises above his head and another out stretches to the side',\n",
       "  'an older male with glasses and gray hair and mustache posing for a martial arts picture',\n",
       "  'an asian man in his karate uniform doing a karate stance',\n",
       "  'a man in a karate pose on a wooden floor',\n",
       "  'an older man is doing a karate pose'],\n",
       " '98773047': ['people sitting at and walking around out front of a sweets cafe',\n",
       "  'people are standing outside of the cafe about to go in',\n",
       "  'people outside of beard papa sweets cafe',\n",
       "  'a busy cafe in an outdoor mall is shown',\n",
       "  'people stand outside a shop at night'],\n",
       " '987907964': ['one brown and white dog chasing a black and white dog through the grass',\n",
       "  'two dogs are running through the grass near a house and trees',\n",
       "  'two cocker spaniels running through the grass',\n",
       "  'the two dogs are running through the grass',\n",
       "  'two dogs are running through a green yard'],\n",
       " '98817947': ['a smiling warehouse worker with a tattoo on his forearm gives the peace sign while surrounded by cardboard boxes',\n",
       "  'a factory worker takes a break from his day to pose for the camera',\n",
       "  'a man takes a quick break from working to pose of a picture',\n",
       "  'a large man in a factory is flashing the peace sign',\n",
       "  'burly warehouse worker giving the victory sign'],\n",
       " '98885561': ['a person is sliding down a slide with a coat jacket gloves and boots on',\n",
       "  'child sliding down an orange and blue slide in the winter',\n",
       "  'a boy in winter clothes slides down an orange slide',\n",
       "  'a child in snow gear sliding down a slide in winter',\n",
       "  'a child slides down an orange slide'],\n",
       " '98944492': ['a man walks on the street with a snow shovel during a snowstorm',\n",
       "  'person with shovel walks through snowstorm in front of cafe',\n",
       "  'a person with red gloves carries a shovel through the snow',\n",
       "  'a person is carrying a shovel walking down a snowy street',\n",
       "  'someone is walking through a snowstorm carrying a shovel'],\n",
       " '989754491': ['a red haired girl making a peace sign is wearing neon green glasses and floaties and playing in the pool with other kids',\n",
       "  'a young girl with goggles and floaties poses for the camera as she plays in a pool',\n",
       "  'a redheaded girl offers the peace sign as she swims in the pool with floaties',\n",
       "  'a girl in a pool wearing goggles and surrounded by other children',\n",
       "  'a girl in green goggles in a pool with three other children'],\n",
       " '989851184': ['a black dog has a dumbbell in his mouth looking at the person wearing blue',\n",
       "  'a black dog holding a weight in its mouth stands next to a person',\n",
       "  'the black dog has a toy in its mouth and a person stands nearby',\n",
       "  'a black dog holds a small white dumbbell in its mouth',\n",
       "  'a black dog has a dumbbell in his mouth'],\n",
       " '990890291': ['asian man in orange hat is popping a wheelie on his bike',\n",
       "  'a man does a wheelie on his bicycle on the sidewalk',\n",
       "  'a man on a bicycle is on only the back wheel',\n",
       "  'a man is doing a wheelie on a mountain bike',\n",
       "  'man on a bicycle riding on only one wheel'],\n",
       " '991459749': ['an obese man and two average sized men sit on a bench with their heads as far back as they will go',\n",
       "  'three men one with his shoes off are asleep on a bench in the park',\n",
       "  'three men taking an afternoon nap',\n",
       "  'three men relaxing on a bench',\n",
       "  'three men sleep on a bench'],\n",
       " '99171998': ['a group of people sit in the snow overlooking a mountain scene',\n",
       "  'five people are sitting together in the snow',\n",
       "  'a group of people sit atop a snowy mountain',\n",
       "  'a group is sitting around a snowy crevasse',\n",
       "  'five children getting ready to sled'],\n",
       " '99458430': ['people standing around laughing at a woman sitting on the ground covered in cake',\n",
       "  'a woman wearing high heels falls backwards as onlookers laugh',\n",
       "  'a woman on the floor and people standing around her',\n",
       "  'woman wearing high heels falls on a cake',\n",
       "  'a woman on the floor covered in cake'],\n",
       " '9950858': ['one police officer standing on the street along with three police officers on horseback',\n",
       "  'a group of people on horses ride down a city street behind others who are walking',\n",
       "  'three mounted police stand in the middle of a street',\n",
       "  'a troupe of police officers atop horses in the city',\n",
       "  'police on horseback at an outside protest'],\n",
       " '9950913': ['a group of people are wearing signs that say on strike while someone is speaking at a booth with the presidential seal',\n",
       "  'a strike is currently going on and there are lots of people',\n",
       "  'a person speaks at a protest on a college campus',\n",
       "  'a woman is speaking at a podium outdoors',\n",
       "  'members of a strike at yale university'],\n",
       " '99679241': ['a gray bird stands majestically on a beach while waves roll in',\n",
       "  'a white crane stands tall as it looks out upon the ocean',\n",
       "  'a tall bird is standing on the sand beside the ocean',\n",
       "  'a large bird stands in the water on the beach',\n",
       "  'a water bird standing at the ocean s edge'],\n",
       " '99804383': ['an older busker in glasses plays an eastern string instrument for a young boy in a striped shirt',\n",
       "  'this is a man in front of a store performing with a little boy standing with a dollar in his hand',\n",
       "  'a older asian man is playing an instrument in front of a young boy on the street',\n",
       "  'an elderly man sits outside a storefront accompanied by a young boy with a cart',\n",
       "  'an elderly gentleman playing a musical instrument on the sidewalk for money']}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Finally we have 2 dictionary \\n    image_features --> this has your image features in 2048 vector\\n    content_dictionary ---> this has your image caption data\\n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Finally we have 2 dictionary \n",
    "    image_features --> this has your image features in 2048 vector\n",
    "    content_dictionary ---> this has your image caption data\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME TO BUILD A LSTM FOR CAPTION GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Dense,LSTM\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
